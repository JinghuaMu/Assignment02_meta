---
title: "Assignment 02"
author: "Jinghua Mu u7457359"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

!(Github repository location)[<https://github.com/henflower/Assignment02_meta>]

## Part I: Data Processing and Analysis

### 1. Read the study raw data and clean

```{r, message = FALSE}
# install packages
library(pacman)
p_load(tidyverse, flextable, metafor, janitor, ggpmisc, patchwork)
# Install the orchaRd package 
## devtools::install_github("daniel1noble/orchaRd", force = TRUE)
library(orchaRd)
```

```{r, message = FALSE}
#read the data of Ckark's research and conceal the index column
current_res <- read_csv("OA_activitydat_20190302_BIOL3207.csv",name_repair = "unique",show_col_types = FALSE) %>%
                     select(-1)
#show context 
head(current_res,3)
```

```{r}
# Change some variables to factor to check that the content is correct
current_res <- current_res %>% mutate(loc = as.factor(loc),species = as.factor(species),
                                         size = as.factor(size),treatment = as.factor(treatment))
summary(current_res)
#Check the distribution of NA in the dataset
current_res %>% summarise(across(everything(), ~sum(is.na(.))))
```

No spelling errors and particularly unusual values were found, but some values were missing. Because our data is species dependent only, the lack of labels `animal ID` does not affect the availability of the data. So we only removed samples that were missing activities.

```{r}
#Check whether the number of NA values affects data availability
table(current_res[!complete.cases(current_res$activity),] %>% select(species,loc))
#Remove missing values
current_res <- current_res[complete.cases(current_res$activity),]
```

6 samples of missing activities were removed, and the distribution of missing values was approximate to the distribution of samples, which we believe does not affect the statistical results.

### 2. Generate statistics and merge files

```{r}
# the path of metadata file to append new infomation
append_path = "ocean_meta_data.csv"
existing_res <- read_csv(append_path,show_col_types = FALSE)
# Get the amount of information we need
glimpse(existing_res)
```

In order to add the data from the *clark et al.* paper to the metadata file, we needed to generate the appropriate statistics $[mean,sd,N]$ and make the variable names consistent with each other. Therefore, we modify the names of treatment groups and change the names of individual variables, and then use `summarise()` and `pivot_longer()` to generate the statistics in the appropriate format. Finally, we read in clark's metadata and use `cbind()` to merge the experimental data with the metadata, and then use `select()` to align the data and write it to the metadata file.

```{r}
# Modify our treatment names to match the reference documentation
levels(current_res$treatment)[levels(current_res$treatment)=="control"] <- "ctrl"
levels(current_res$treatment)[levels(current_res$treatment)=="CO2"] <- "oa" 
# calculate mean,SD and N of activity across each species and treatment 
current_sta <- current_res %>% group_by(treatment,species) %>% summarise(mean = mean(activity),sd = sd(activity),n = n(),.groups = "drop") %>% 
       pivot_wider(names_from = treatment,names_glue = "{treatment}.{.value}",values_from = c(mean,n,sd))
```

```{r}
# the path of supplemental information about clark's research
current_info <- read_csv("clark_paper_data.csv",show_col_types = FALSE)
# using cbind() to extend supplemental information to each experimental group
append_info <- cbind(tibble(current_info),current_sta)
# colnames(append_info)
#Fix problem with the order and partial naming of columns was found
append_info <- append_info %>%rename(Species = species) %>% select(colnames(existing_res))
```

```{r}
# write_lines("",file = append_path,append = TRUE)
# write_csv(append_info, file = append_path, append = TRUE,col_names = FALSE)
rm(list = ls())
```

### 3. Generate lnRR and sampling variance

In the previous section we found a problem with data reading for metadata, some numeric variables are read in character format, and we want to check the non-numeric content of these variables and correct all the variables into suitable format.

```{r, warning = FALSE}
# Use *janitor* to fix column names for easy selection
meta_data <- read_csv("ocean_meta_data.csv",show_col_types = FALSE) %>% janitor::clean_names()
ini_size = nrow(meta_data)
# Some numeric variables are read in chr type, cheack the non-numeric element
unique(meta_data$pub_year_if[is.na(as.numeric(meta_data$pub_year_if))])
unique(meta_data$x2017_if[is.na(as.numeric(meta_data$x2017_if))])
```

```{r}
#Correct "-" to NA by forced type conversion. Correct some variables to factor type
meta_data[meta_data == "-"] <- NA
meta_data$cue_stimulus_type[is.na(meta_data$cue_stimulus_type)] <- "None"
meta_data <- meta_data %>% mutate(pub_year_if = as.numeric(pub_year_if), x2017_if = as.numeric(x2017_if),
                        effect_type = as.factor(effect_type), climate_fish_base = as.factor(climate_fish_base),
                        env_cue_stimulus = as.factor(env_cue_stimulus), cue_stimulus_type = as.factor(cue_stimulus_type),
                        species = as.factor(species))
# Check the dataset again
summary(meta_data)
```

The mean values of both groups were found to vary greatly between experiments. We made an attempt to convert them to **Log response ratio (**$lnRR$**)**.

```{r}
meta_RR <- metafor::escalc(measure = "ROM", data = meta_data,m1i = oa_mean, m2i = ctrl_mean, 
      sd1i = oa_sd, sd2i = ctrl_sd,n1i = oa_n, n2i = ctrl_n,var.names = c("lnRR", "VRR"))
```

The reason for getting NA is that $lnRR = ln\frac{\bar{X_1}}{\bar{X_2}}$, which cannot be calculated when the signs of the two response variables do not agree. We believe that the reason for the inconsistency in the sign of the two variables is that: **part of the experiments screened by metadata is to measure the change from particular baseline for each treatment.**

```{r}
# Most of NA has *change/rate* in behavioural_metric 
meta_RR[!complete.cases(meta_RR$lnRR),] %>% select(study,behavioural_metric, ctrl_mean,oa_mean,lnRR)
```

We further counted the keyword occurrences of the study in which the NA experiments were found. The results showed that 104 of the 109 experiments had keywords with **change** and **rate**. We believe that such experiments do not show their effect sizes correctly by response ratio ($lnRR = ln\frac{\bar{X_1}-M}{\bar{X_2}-M} \neq \frac{\bar{X_1}}{\bar{X_2}}$).

```{r}
# get all the study id of NA values
doubtful_study <- unique(meta_RR[!complete.cases(meta_RR$lnRR),]$study)
# number of experiments matched keyword in doubtful study
num_match <- meta_data %>% filter(study %in% doubtful_study) %>% 
         filter(grepl("change|Change|rate|Rate",behavioural_metric)) %>% nrow()
# number of experiments of all study including NA value
num_in_study <- meta_data %>% filter(study %in% doubtful_study) %>% nrow()
print(paste("Num of rows matching keywords:",num_match,",Num of experiments:",num_in_study))
```

Therefore, we chose to **label** the experiments in these studies and all other experiments with the keyword **"change"** to detect the effect size bias between these and other experiments. Also, we removed those experiments where $lnRR$ was NA.

```{r}
# Remove NA and mark  experiment with change or in "doubtful study"
meta_RR <- meta_RR %>%  filter(!is.na(lnRR)) %>% 
         mutate(measure_change = (study %in% doubtful_study)|grepl("change|Change",behavioural_metric) )
```

We test whether the obtained $lnRR$ and its variance $V_{lnRR}$ have a appropriate distribution and size.

```{r fig1, fig.cap="Distribution of lnRR and VRR before sample deletion"}
# plot distribution of lnRR and VRR
meta_RR %>% select(lnRR, VRR) %>% pivot_longer(cols = lnRR:VRR, names_to = "type", values_to = "value") %>%
      ggplot(aes(x = 1,y = value)) + geom_violin() + facet_wrap(~type, scale = "free") + ggtitle("Distribution of lnRR and VRR before sample deletion") + xlab(NULL)
```

The violin plot Fig \@ref(fig:fig1) shows that the shape of $lnRR$ is consistent with its symmetrical distributed nature, but the values at both ends are large, and the extreme values of the response ratio reach the level of $e^{14}$, which is unimaginable but we cannot deny its plausibility. However, the shape of the \$V\_{lnRR}\$ is extremely anomalous, and there are individual extremely large \$V\_{lnRR}\$. we consider the results of experiments with too large variance relative to the effect values to be unreliable, and we therefore choose to remove these experiments.\
To prevent bias, we used \$ (sd(lnRR) \> 2\|lnRR\|) \cap (sd(lnRR) \> ln(1.5))\$ as a criterion.

```{r tags=c()}
# filter unreliable(have unreliable sampling variance) experiments
meta_RR %>% filter( (sqrt(VRR) > 2*abs(lnRR)) & (sqrt(VRR)> log(1.5) )) %>% select(study,ctrl_n:VRR) %>% head(10)
```

```{r}
# delete unreliable(have unreliable sampling variance) experiments
meta_RR <- meta_RR %>% filter((sqrt(VRR) <= 2*abs(lnRR)) | (sqrt(VRR) <= log(1.5) ))
print(paste("Initial number of experiment:",ini_size,",after screen:",nrow(meta_RR)))
# Write the processed file to a csv file
write_csv(meta_RR,"meta_RR.csv")
```

By screening, we removed 97 of the 824 experiments, which reduced the validity of our meta-analysis but was more conducive to obtaining reliable results.

After removing some of the samples, we redraw the distributions of $lnRR$ and \$V\_{lnRR}\$ and show the experiments measuring the degree of change in another color.

```{r fig2, fig.cap="Distribution of lnRR and VRR after sample deletion"}
meta_RR %>% mutate(sd_lnRR = sqrt(VRR)) %>% select(lnRR, sd_lnRR, measure_change) %>% pivot_longer(cols = c(lnRR,sd_lnRR), names_to = "type", values_to = "value") %>% ggplot(aes(x = 1,y = value)) + geom_violin() +geom_jitter(alpha = 0.3,aes(color = measure_change))+ facet_wrap(~type, scale = "free") + xlab(NULL) + plot_annotation(
  title = "Distribution of lnRR and VRR after sample deletion")
```

As we can also see from the violin plot Fig \@ref(fig:fig2) added measurement method, the experiments that measure the ratio of change generally have large effect values and sampling variance. And even after removing the experiments with low confidence, some of the effect values are still too large and the sampling variance still has some extreme values.

we observe the difference in distribution density between the two experimental data measurement types further through density plots.

```{r fig3, fig.cap="Density of lnRR and VRR between two measurement methods"}
meta_RR %>% mutate(sd_lnRR = sqrt(VRR)) %>% select(lnRR, sd_lnRR, measure_change) %>% pivot_longer(cols = c(lnRR,sd_lnRR), names_to = "type", values_to = "value") %>% 
      ggplot(aes(x = value)) + geom_density(aes(color = measure_change)) + facet_wrap(~type, scale = "free") + ggtitle("Density of lnRR and VRR between two measurement methods")
```

By plotting the density Fig \@ref(fig:fig3), we find that experiments measuring the rate of change do have higher absolute effect values and show greater sampling variance.

## Part II: Meta Analysis of OA Research

### 4. Overall meta-analysis

Before fitting the Multi-level meta-analytic model, we need to know the **composition of the experimental design** of the different studies in order to better design the random effect factors.

```{r fig4, fig.cap="Composition of experimental designs across all studies"}
# Read processed meta-analysis data
meta_RR <- read_csv("meta_RR.csv", show_col_types = FALSE)
# Fix a UTF-8 encoded label of species
meta_RR$species[grepl("Hippocampus",meta_RR$species)] = "Hippocampus guttulatus"

#draw fig4
meta_RR %>% group_by(study) %>% 
    summarise(n_climate = length(unique(climate_fish_base)), n_species = length(unique(species)), n_life_stage = length(unique(life_stage)),n_stimulus= length(unique(cue_stimulus_type))) %>% 
    pivot_longer(cols = -1,names_to = "type", values_to = "value") %>% 
    ggplot(aes(x = value)) + geom_histogram(binwidth = 1) + geom_text(aes(label=as.character(..count..)),stat="bin",binwidth=1,vjust=-0.5) + 
    facet_wrap(~type,scale = "free",nrow = 1) + plot_annotation(
  title = "Composition of experimental designs across all studies")
```

We counted the number of materials and study methods for each study. It can be found in Fig \@ref(fig:fig4) that almost every study studied only one zone (with only one exception) and one fish life stage.Therefore we assume that climate and life stage do not affect the variance composition of study. Instead, the internal variance of each study may be influenced by the method and the number of species it uses.

At the same time, we also counted the life stages involved in each species, the types of stimuli they received, and how many studies were involved.

```{r fig5, fig.cap="Distribution of the number of experiments and methods involved in species"}
meta_RR %>% group_by(species) %>% 
    summarise(n_study = length(unique(study)), n_life_stage = length(unique(life_stage)),n_stimulus= length(unique(cue_stimulus_type))) %>% 
    pivot_longer(cols = -1,names_to = "type", values_to = "value") %>% 
    ggplot(aes(x = value)) + geom_histogram(binwidth = 1) + geom_histogram(binwidth = 1) + geom_text(aes(label=as.character(..count..)),stat="bin",binwidth=1,vjust=-0.5) + 
    facet_wrap(~type,scale = "free") + plot_annotation(
  title = "Distribution of the number of experiments and methods involved in species")
```

We found in Fig \@ref(fig:fig5) that each species was mostly studied at only one of its growth stages. Most species appeared in only one study, with little overlap of species between studies. And nearly half of the species in all experiments were studied under multiple study instruments (stimulus).

Combining Fig \@ref(fig:fig4) and Fig \@ref(fig:fig5), we found that most of the published studies mostly used only one growth stage of a species as a sample, most of the studies using multiple species involved only one zone, and only one to two study instruments (stimulus) were used in each study. Therefore, we can assume that most of the factors affecting the within-study variance are differences in the stimuli used and the number of studies per study.

In addition, we can make Q-Q plots to test whether the mean distribution of $lnRR$ in each factor conforms to a normal distribution, thereby confirming whether these effects lead to non-random differences. First, we make Q-Q plots for different effect_types to better show the deviation of the overall distribution of $lnRR$ from the normal distribution.

```{r fig6, fig.cap="Q-Q plot of lnRR for three different study effect types"}
meta_RR %>% ggplot(aes(sample = lnRR)) + stat_qq() + stat_qq_line() + facet_wrap(~effect_type,scale = "free") +  plot_annotation(
  title = "Q-Q plot of lnRR for three different study effect types")
```

As seen in Fig \@ref(fig:fig6), each reported response type deviates significantly from the normal distribution and shows strong thick tails, implying the presence of many extreme values of $lnRR$. Also, we can see that the Q-Q plot fits to $lnRR$ are centered at 0, which implies that the general effect values are distributed approximately symmetrically at both ends of 0. This is because each study has different values measured by the study instruments and the effect values have different meanings for the species. At the same time, however, the graph also implies that we can quantify absolute effect sizes by counting the absolute value of $lnRR$.

```{r fig7, fig.cap="Q-Q plot of mean lnRR for all experimental factors"}
para_distribution <- function(data,paras){
   plot_data <- tibble()
   for (i in paras){
      c <- ensym(i)
      para_data <- data %>% group_by({{c}}) %>% summarise(mean_lnRR = mean(lnRR)) %>% 
          select(mean_lnRR) %>% mutate(parameter = as.character(i))
      plot_data <- rbind(para_data,plot_data)
   }
   plot_data %>% ggplot(aes(sample = mean_lnRR)) + stat_qq() + stat_qq_line() + facet_wrap(~parameter,scale = "free") + ylab("mean of abslute lnRR") +  plot_annotation(
  title = "Q-Q plot of mean lnRR for all experimental factors")
}

para_distribution(meta_RR,c("species","study","climate_fish_base","cue_stimulus_type","year_online","authors"))
```

At the same time, the Q-Q plots for different experimental factors Fig \@ref(fig:fig7) show that, except for the effect size of the stimulus type, which conforms to a normal distribution centered at 0, the results of other experimental factors deviate significantly from the normal distribution.

We therefore consider that the main experiment random effect belongs to the **study**, while the **stimulus type** as different experimental methods in a single **study** leads to different between-group variance, while other variance is contributed by differences in conditions between experiments(contain the use of **species**) and the experiment itself. From this we build a meta-analytic model.(Because the ecological niche in which a species is located may determine the type of stimulus it receives)

```{r}
meta_RR <- meta_RR %>% group_by(study) %>% mutate(exp_id = 1:n()) %>% ungroup()
```

```{r}
# Multi-level meta-analytic model
MLMA <- metafor::rma.mv(lnRR ~ 1, V = VRR, method = "REML", 
            random = list( ~1|study/cue_stimulus_type/species/exp_id),
            dfs = "contain", test = "t", data = meta_RR)
```

```{r}
summary(MLMA)
```

Because 1. we do not set the independent variable corresponding to $lnRR$ but only measure the mean value\
2. the difference between the extreme values of $lnRR$ is too large\
3. the difference of using species、experimental methods and experimental variable between different experiments is large\
4. No consistent measurements across experiments

Our results are not significant with p-value `r round(as.numeric(MLMA$pval),3)`. Although our 95% confidence interval contains 0, the results obtained are still somewhat informative, with the average $lnRR$ being `r round(as.numeric(MLMA$b),3)`.

```{r}
p_MLMA <- predict(MLMA, transf = exp)
p_MLMA
```

We convert $lnRR$ back to Respose ratio $(RR)$, and wce expect a 95% chance of a true RR falling between `r round(p_MLMA$ci.lb,3)` - `r round(p_MLMA$ci.lb,3)`.

Measures of heterogeneity in effect size estimates across studies $I^2$:

```{r table1, echo=TRUE, eval=TRUE, tab.cap="I^2 of overall meta analysis model"}
## Calculate I2
library(orchaRd)
i2_vals <- orchaRd::i2_ml(MLMA)
i2 <- tibble(type = firstup(gsub("I2_", "", names(i2_vals))), I2 = round(i2_vals,4))
flextable(i2) %>%
   align(part = "header", align = "center") %>%
   compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>%
   compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")), as_b("(%)")))
```

Surprisingly, the random effect explains 100% of the variance, while the sampling variance is too small to show its proportion because it is relatively too small. This suggests that our data are extremely heterogeneous for the same reasons as above, both due to high variability in experimental design and measurements between experiments.It can be seen that the variance between studies was decomposed to account for approximately 0. While the stimulus type explained `r as.numeric(i2[i2$type == "Study/cue_stimulus_type","I2"])`% of the variance and species explained `r as.numeric(i2[i2$type == "Study/cue_stimulus_type/species","I2"])`%of the variance.

We also tried the same meta-analysis using the absolute value of $lnRR$, although this would lead to random effects not meeting the assumption of a normal distribution and the t-test would be inaccurate.

```{r}
# Multi-level meta-analytic model
meta_RR <- meta_RR %>% mutate(abs_lnRR = abs(lnRR))
MLMA2 <- metafor::rma.mv(abs_lnRR ~ 1, V = VRR, method = "REML",
           random = list(~1|study/cue_stimulus_type/species/exp_id),
            dfs = "contain", test = "t", data = meta_RR)
MLMA2
```

Fitting to the absolute value of $lnRR$ yielded a very high estimate mean value of `r round(as.numeric(MLMA2$b),3)`, with a very small p-value of `r round(as.numeric(MLMA2$pval),3)`. This shows that the absolute effect size of our study is high. That mean the response ratio between two treatment groups across experiments is `r round(exp(as.numeric(MLMA2$b)),3)`.

Measures of heterogeneity in effect size estimates across studies $I^2$ for absolute $lnRR$:

```{r table2, echo=TRUE, eval=TRUE, tab.cap="I^2 of overall meta analysis model in abs(lnRR)"}
## Calculate I2
library(orchaRd)
i2_vals <- orchaRd::i2_ml(MLMA2)
i2 <- tibble(type = firstup(gsub("I2_", "", names(i2_vals))), I2 = round(i2_vals,4))
flextable(i2) %>%
   align(part = "header", align = "center") %>%
   compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>%
   compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")), as_b("(%)")))
```

Using absolute values does not explain the variance any better. Because of the large variation between our different experiments, it still leads to 100% heterogeneity, but the variance explained by study comes back to a more reasonable level.

Finally, we make a meta-analysis of the overall forest plot.

```{r fig8, fig.cap="Orchard plot of lnRR across all study"}
orchaRd::orchard_plot(MLMA, mod = "1", group = "study", data = meta_RR,
    xlab = "log(Response ratio) (lnRR) across all studies", angle = 45)
```

In Fig \@ref(fig:fig8), it can be seen that the results are consistent with MLMA1, the mean value of $lnRR$ is positive, and the 95% confidence interval intersects with 0.

### 5. Analysis of biological and methodological factor

First, whether we fit biological factors can better explain the changes in $lnRR$.\
The effect of ocean acidification may be different in different habitats (climate zones) because of natural differences in CO2 concentration in the ocean due to temperature differences in different habitats.

```{r}
# Experiments didn't mark habitats were remove 
meta_RR_climate <- meta_RR %>% filter(climate_fish_base != "Not provided")
MLMR1 <- metafor::rma.mv(lnRR ~ climate_fish_base-1, V = VRR, method = "REML", 
                         random = list(~1|species, ~1|study/exp_id),
                         dfs = "contain", test = "t", data = meta_RR_climate)
MLMR1
```

Although all p-values obtained from the fit were greater than 0.1, the mean value of lnRR showed large differences between habitats, ranging from `r round(MLMR1$b[2],3)`, the highest in deep water, to `r round(MLMR1$b[4],3)`, the lowest in subtropic water. This confirms that **ocean acidification has different effects in different habitats**.

Giving the orchard plot for different habitats.

```{r fig9, fig.cap="Orchard plot for different habitats"}
orchaRd::orchard_plot(MLMR1, mod = "climate_fish_base", group = "study", data = meta_RR_climate,
    xlab = "log(Response ratio) (lnRR)", angle = 45)
rm(meta_RR_climate)
```

From Fig \@ref(fig:fig9), it can be seen that the mean values of most of the habitats basically fall at zero. While some of the experiments with large sample sizes and some of the experiments with more extreme values cause the mean values to deviate from zero.

At the same time, it is widely believed that ocean acidification may have different effects on different life stages of fish. We therefore fit the mean of $lnRR$ for different life stages of the fish.

```{r}
# Experiments didn't mark life stage were remove 
meta_RR_stage <- meta_RR %>% filter(life_stage != "Not provided")
MLMR2 <- metafor::rma.mv(lnRR ~ life_stage -1, V = VRR, method = "REML", 
                         random = list(~1|species, ~1|study/exp_id),
                         dfs = "contain", test = "t", data = meta_RR_stage)
MLMR2
```

The fit to the life stage showed significant differences in the mean values of $lnRR$ obtained for different life stages. The larvae group has the largest mean value of $lnRR$ with `r round(MLMR2$b[3],3)`. In contrast, the adult and juvenile groups showed negative mean effect values. This indicates that **fish may be affected by different species at different life stages**.

Giving the orchard plot for different life stages.

```{r fig10, fig.cap="Orchard plot for different life stages"}
orchaRd::orchard_plot(MLMR2, mod = "life_stage", group = "study", data = meta_RR_stage,
    xlab = "log(Response ratio) (lnRR)", angle = 45)
rm(meta_RR_stage)
```

Fig \@ref(fig:fig10) confirms our previous conclusions, and we find several studies with great precision focused on the larvae domain, which seems to be related to the large positive effect values in this domain.

```{r}
# Experiments didn't mark climate and life stage were remove
meta_RR_cs <- meta_RR %>% filter(life_stage != "Not provided" & climate_fish_base != "Not provided")
MLMR3 <- metafor::rma.mv(abs_lnRR ~ climate_fish_base + life_stage -1, V = VRR, method = "REML", 
                         random = list(~1|species, ~1|study/exp_id),
                         dfs = "contain", test = "t", data = meta_RR_cs)
MLMR3
```

We use `orchaRd::r2_ml()` to see how well the three models explain variance.

```{r table3, echo=TRUE, eval=TRUE, tab.cap="Variance explained by different publication bias meta-analysis models"}
R2 <- as.data.frame(orchaRd::r2_ml(MLMR1))
R2 <- cbind(R2,as.data.frame(orchaRd::r2_ml(MLMR2)))
R2 <- cbind(R2,as.data.frame(orchaRd::r2_ml(MLMR3)))
colnames(R2) <- c("climate","life stage","climate + life stage")
knitr::kable(R2,caption = "Variance explained by different publication bias meta-analysis models")
```

As can be seen from the table \@ref(tab:table3), using two variables at the same time explains the variance between experiments relatively better. However, due to the high variation between experiments, the fixed factor still only explains the variance of `r round(R2["R2_marginal", "climate + life stage"],3)`.

Because we believe that using the absolute values of the effect sizes better reflects the magnitude of the differences and the "value" of the study. Therefore, we used the average absolute value of $lnRR$ and the sample size of the study to make a heat map of these two fixed factors. Also, we used a linear fit to see if the study area with stronger effects was more popular.

```{r fig11, fig.cap="Heatmap of mean(abs(lnRR)) to two fixed effects "}
#Calculate the average lnRR and sample size grouped by habitat and life stage
heat_data <- meta_RR_cs %>% group_by(climate_fish_base, life_stage) %>% 
    summarise(mean_abs_lnRR = mean(abs_lnRR), n_sam = n(), .groups = "drop") %>% filter(n_sam > 10) %>% 
    mutate(heat_label = paste("n =",n_sam,"\n",round(mean_abs_lnRR,3)))

heat_data %>% ggplot(aes(x= climate_fish_base ,y= life_stage,fill= mean_abs_lnRR, label = heat_label)) + geom_tile() + geom_text() + scale_fill_gradient2(low = "white", high = "red") + xlab("climate") + ylab("life stage") + labs(fill = "mean(abs(lnRR))") + ggtitle("Heatmap of mean(abs(lnRR)) to two fixed effects")
```

Fig \@ref(fig:fig11) confirms that the magnitude of the absolute effect size varies significantly between the climate and life stages, and also shows that blocks with larger absolute effect sizes have larger sample sizes.

```{r fig12, fig.cap="Linear fit to the mean absolute value of lnRR and the number of experiments"}
heat_data %>% ggplot(aes(x = mean_abs_lnRR, y = n_sam)) + geom_smooth(method = "lm") + geom_point() + xlab("Mean(abs(lnRR)) for each block in heatmap") + ylab("Mean sample size") + ggtitle("Linear fit to the mean absolute value of lnRR and the number of experiments")
```

Fig \@ref(fig:fig12) reveals that there is a strong positive correlation between the absolute value of the effect size and the number of studies for different "research fields". It can be argued that **researchers prefer to conduct research in areas where it is easy to find significant effects**.

Finally, we explore whether the sample size has an effect on the size of the effect size, that is, the relationship between sample size and $|lnRR|$.

```{r fig13, fig.cap="The relationship between sample size and abs(lnRR)"}
meta_RR %>% filter(average_n < quantile(meta_RR$average_n,0.99)) %>% ggplot(aes(x = average_n, y = abs_lnRR, color = VRR)) + geom_point() + geom_smooth(method = lm) + scale_color_gradientn(colours = topo.colors(10)) + ggtitle("The relationship between sample size and abs(lnRR)") 
```

From Fig \@ref(fig:fig13), it can be clearly seen that regions with low sample sizes tend to have higher absolute values of effect sizes, and also generally have larger sampling variances. Therefore, it can be seen that a **low sample size will lead to a greater possibility of bias** in the results of the experiment, and the reliability is often low.

### 6. Analysis of publication practices factor

Before demonstrating the effect of study precision $(1/\sqrt{lnRR})$ on effect size, we first grouped the precision for better observation.

```{r fig14, fig.cap="Relationship between lnRR and precision under different precision groups"}
# use case_when() to divide samples into groups
meta_RR <- meta_RR %>% mutate(precision = 1/sqrt(VRR)) %>%  mutate(
    exact = case_when(
        precision < 5 ~ "low",
        precision >= 5 & precision < 30 ~ "normal",
        precision >= 30 & precision < 500 ~ "exact",
        precision >= 500 ~ "extra_exact",
  )) %>% mutate(exact = fct_relevel(as.factor(exact),"low","normal","exact"))
meta_RR %>% ggplot(aes(y = precision, x = lnRR)) + geom_point() + facet_wrap(~exact, scale = "free") + plot_annotation(
  title = "Relationship between lnRR and precision under different precision groups")
```

It can be seen from Fig \@ref(fig:fig14) that there are **some experiments with extremely high precision** in ocean acidification research, which means that the sampling variance of these experiments is extremely small, and these studies have strong effect sizes. In other groups, we found that the distribution of $lnRR$ was more discrete, but its variation range gradually decreased with the increase of precision.

We calculate the proportion of the sample size in each group to the population.

```{r table4, echo=TRUE, eval=TRUE, tab.cap="The number and proportion of samples with different precision levels"}
precision_num <- meta_RR %>% count(across(exact)) %>% mutate(ratio = round(n/nrow(meta_RR),3))
knitr::kable(precision_num,caption = "The number and proportion of samples with different precision levels")
```

From Table \@ref(tab:table4), we can find that the number of samples in the extra_exact grouping is only `r precision_num[4,2]`. And the number of fact and extra_exact groupings together only accounts for `r sum(precision_num[3,3],precision_num[4,3])`% of the total number of samples, so removing these samples *does not affect our observation of the overall*.

Publication bias of study was examined by plotting funnel plots.

```{r fig15, fig.cap="Funnel plot of lnRR and study precision"}
metafor::funnel(x = meta_RR$lnRR, vi = meta_RR$VRR, yaxis = "seinv",
    digits = 2, level = c(0.1, 0.05, 0.01), shade = c("#e9fbff", "#dcf0ef", "#bac8cc"),ylim = c(0.01,30),xlim = c(-3,3),
    las = 1, xlab = "Correlation Coefficient (r)", atransf = exp, legend = TRUE, pch =20)
```

We found no publication bias in the areas where the studies had higher precision. However, we found gaps at the bottom of the funnel plot Fig \@ref(fig:fig15), implying that **studies with lower precision tended to be biased toward publishing results with more pronounced effects**, that is, results with larger absolute values of effect sizes.

To this end, we plot a linear fit of the sampling variance to the absolute value of $lnRR$ to examine the relationship between the absolute value of the effect size and the sampling variance/study precision.

```{r fig16, fig.cap="Linear fitting of VRR to the absolute value of lnRR"}
library(ggpmisc)
anti_funnel <- function(data, diff_color){
    ggplot(data, aes(y = abs_lnRR, x = VRR, color = {{diff_color}})) + geom_point() + geom_smooth(method = lm, formula = y~x) +
    labs(y = "Absolute ln response ratio(abs(lnRR)）", x = "Sampling Variance of lnRR") +
    stat_poly_eq(use_label(c("eq","R2", "P"), sep = "*\"; \"*"),
    formula = y~x , label.x = 8, parse = TRUE) +
    theme_classic() + ggtitle("Linear fitting of VRR to the absolute value of lnRR")
}
# NULL means not to apply different colors to the two measurements of the data
meta_RR %>% filter(VRR < quantile(meta_RR$VRR,0.95)) %>% anti_funnel(NULL)
```

A fit to the post-95% sampling variance can be found to show a strong positive correlation. This suggests that **studies with larger sampling variances tend to publish results with larger absolute values of effect sizes**.

At the same time, according to Fig \@ref(fig:fig16), we believe that the effect size distribution of the two different data measures is different, and therefore we believe that their responses to VRR may also be different. Therefore, we fit the same data separately for the two types of data.

```{r fig17, fig.cap="Linear fitting of VRR to the absolute value of lnRR to different measure type"}
meta_RR %>% filter(VRR < quantile(meta_RR$VRR,0.95)) %>% anti_funnel(measure_change)
```

According to Fig \@ref(fig:fig17), we obtain very interesting results. The study measuring the change quantity has a higher intercept of the absolute value of $lnRR$ , while the study measuring the non-change quantity has a smaller intercept of the absolute value of $lnRR$, yet has a higher slope. This is partly because the measurement of change itself leads to larger effect sizes, and partly because the measurement of such effect sizes leads to larger sampling variance, making the change in effect sizes less pronounced.

Meanwhile, we perform a meta-analysis of inverse sampling variance \$1/V\_{lnRR}\$ versus $lnRR$. To filter too large values, we choose to refer to table \@ref(tab:table4) to take the 95% quantile.

```{r}
metareg_v <- metafor::rma.mv(lnRR ~ inverse_VRR, V = VRR, random = list(~1 | study/exp_id),
    test = "t", dfs = "contain", data = meta_RR %>% mutate(inverse_VRR = 1/VRR) %>% filter(inverse_VRR <= quantile(inverse_VRR, 0.95)))
metareg_v
```

The results show that \$1/V\_{lnRR}\$ as a fixed effect explains well the effect on lnRR with a p-value of `r round(metareg_v$pval[2],3)` less than 0.05, an estimated slope of `r round(metareg_v$beta[2],3)`(95% CI: [`r round(metareg_v $ci.lb[2],3)`,`r round(metareg_v$ci.ub[2],3)`].

Next we look at the time-lag bias in ocean acidification studies.

```{r fig18, fig.cap="Variation of effect size over time"}
ggplot(meta_RR, aes(y = lnRR, x = year_online, size = 1/sqrt(VRR))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Fisher's Z-transformed Correlation Coefficient (Zr)", size = "Precision (1/SE)") +
    geom_hline(yintercept = 0,col = "blue", linetype = 2)+
    theme_light() + ggtitle("Variation of effect size over time")
```

It can be clearly seen from Fig \@ref(fig:fig18) that the mean value of the **effect size gradually decreases with the publication year** **and gradually converges to close to zero**.This shows that with the deepening of research, the effect of ocean acidification is gradually revised and reduced.

We can use the absolute value of the effect size to better observe the convergence of this effect size.

```{r fig19, fig.cap="Variation of absolute effect size over time"}
ggplot(meta_RR, aes(y = abs_lnRR, x = year_online, size = 1/sqrt(VRR))) + geom_boxplot(aes(group = year_online)) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Fisher's Z-transformed Correlation Coefficient (Zr)", size = "Precision (1/SE)") +
    geom_hline(yintercept = 0,col = "blue", linetype = 2)+ coord_cartesian(ylim = c(0, 8))+
    stat_summary(fun="mean", geom="point", shape=20, size=2.5, color="blue", fill="blue",alpha=0.7) +
    theme_classic() + ggtitle("Variation of absolute effect size over time")
```

With the help of the box plot in Fig \@ref(fig:fig19), we found that the absolute value of the effect size decreased in a wavy shape with the year. In 2020, the effect sizes obtained by basically all studies were around 0.

Therefore, we can build a meta-analysis model with time as a fixed factor.

```{r}
meta_RR <- meta_RR %>% mutate(year_c = year_online - mean(year_online))
metareg_time <- metafor::rma.mv(lnRR ~ year_c, V = VRR, random = list(~1 | study/exp_id),
    test = "t", dfs = "contain", data = meta_RR)
metareg_time
```

The results show a clear effect of year change on the effect size with a slope of `r round(metareg_time$beta[2],3)` and a 95% confidence interval of `r round(metareg_time $ci.lb[2],3)` to `r round( metareg_time $ci.ub[2],3)`. This means that the mean of the effect size decreases on average by `r round(metareg_time$beta[2],3)` per year. Also the average effect value after removing the year effect is still 0.1447 greater than 0.

Finally, we combined time and study precision simultaneously as fixed effects for meta-analysis.To filter too large values, we choose to refer to table \@ref(tab:table4) to take the 99% quantile.

```{r}
metareg_publish <- metafor::rma.mv(lnRR ~ year_c + precision, V = VRR, random = list(~1 | study/exp_id),
    test = "t", dfs = "contain", data = meta_RR %>% filter(precision < quantile(meta_RR$precision,0.99)))
metareg_publish
```

The results show that both fixed effects are highly significant. where the slope of year is `r round(metareg_publish$beta[2],3)` (95% CI [`r round(metareg_publish$ci.lb[2],3)`,`r round(metareg_publish$ci.ub[2 ],3)`]), the slope of the precision is `r round(metareg_publish$beta[3],3)` (95% CI [`r round(metareg_publish$ci.lb[3],3)`, `r round(metareg_publish$ ci.ub[3],3)`]). And the mean of the effect size obtained by removing the two fixed effects is `r round(metareg_publish$beta[1],3)` (95% CI [`r round(metareg_publish$ci.lb[1],3)`,`r round(metareg_publish $ci.ub[1],3)`]). We believe that the decay in effect values over time is due to **initial studies that exaggerated the effect of the study**, leading to an academic consensus that ocean acidification has a strong effect on species, while a growing number of subsequent experiments have gradually refuted the previously exaggerated effect sizes.

```{r table5, echo=TRUE, eval=TRUE, tab.cap="Variance explained by different publication bias meta-analysis models"}
R22 <- as.data.frame(orchaRd::r2_ml(metareg_v))
R22 <- cbind(R22,as.data.frame(orchaRd::r2_ml(metareg_time)))
R22 <- cbind(R22,as.data.frame(orchaRd::r2_ml(metareg_publish)))
colnames(R22) <- c("precision","year","year+precision")
knitr::kable(R22, caption = "Variance explained by different publication bias meta-analysis models")
```

As can be seen from the table \@ref(tab:table5), using both time and precision **better explains** the variance between experiments. The two fixed effects explained `r 100*R22[1,3]`% of the variance, and were more powerful than biological effects.

Lastly, we investigated whether the impact factor influenced the publication of the study.

```{r}
study_info <- meta_RR %>% group_by(study) %>% mutate(mean_VRR = mean(VRR), species_usage = length(unique(species)), mean_lnRR = mean(lnRR), method_usage = length(unique(cue_stimulus_type))) %>% ungroup() %>%  select(study,year_online, pub_year_if,mean_VRR, effect_type, average_n, species_usage, method_usage,mean_lnRR) %>% distinct()
study_info <- study_info %>% mutate(if_range = c("0-20%","20-40%","40-60%","60-80%","80-100%")[findInterval(study_info$pub_year_if,c(0,quantile(study_info$pub_year_if,0.2,T),quantile(meta_RR$pub_year_if,0.4,T),quantile(study_info$pub_year_if,0.6,T),quantile(study_info$pub_year_if,0.8,T),100),all.inside = T)] )
```

First, we observed whether effect_type affected the impact factor of the study.

```{r fig20, fig.cap="Distribution of effect types studied on different IF intervals"}
study_info %>% drop_na() %>% 
ggplot(aes(x= if_range,fill=effect_type)) + geom_bar(aes(color=effect_type), position = "fill")+scale_y_continuous(labels = scales::percent) + ggtitle("Distribution of effect types studied on different IF intervals")
```

From Fig \@ref(fig:fig20) it can be seen that the majority of studies with no effect were published in journals with low impact factor quartiles, while the majority of studies with strong effects were published in journals with high impact factor quartiles, which seems to **indicate the acceptance bias of the journals**.

```{r fig21, fig.cap="Distribution of other research characteristics in different IF intervals"}
study_info %>% drop_na() %>% mutate(precision = 1/sqrt(mean_VRR)) %>% pivot_longer(c(mean_lnRR,average_n,precision),names_to = "trait", values_to = "value") %>% 
ggplot(aes(y= if_range,x = value)) + geom_boxplot() + facet_wrap(~trait,scale = "free",nrow = 3) +  plot_annotation(
  title = "Distribution of other research characteristics in different IF intervals")
```

In the analysis of Fig \@ref(fig:fig21) impact factors in other study characteristics, we found that the precision of the studies did not change significantly as the quantile of impact factors rose, while the range of variation in effect size increased overall and the sample size of the studies increased. This seems to imply that **articles with high impact factors do not require as much precision as they do sample size**.

Finally, we examine whether the number of species and methods used in published studies increases as the impact factor increases.

```{r fig22, fig.cap="Count of method and species usage in different IF interval"}
study_info %>% drop_na() %>% pivot_longer(c(species_usage,method_usage),names_to = "trait", values_to = "value") %>% ggplot(aes(y= if_range,x = value)) + geom_count() + facet_wrap(~trait,scale = "free",ncol = 2) +  plot_annotation(
  title = "Count of method and species usage in different IF interval")
```

Fig \@ref(fig:fig22) shows that the **number of methods used in the study did not increase with increasing impact factors**, while the number of species used increased overall.

A more pronounced effect of different publication factors on the absolute value of the effect size was found from Fig \@ref(fig:fig17), Fig \@ref(fig:fig19) and Fig \@ref(fig:fig22). To represent the effect of different publication bias manifold on the absolute value of effect size, we fit a meta-analytic model to the absolute value of $lnRR$.

```{r}
metareg_all <- metafor::rma.mv(abs_lnRR ~ year_c + pub_year_if + precision , V = VRR, random = list(~1 | study/exp_id),
    test = "t", dfs = "contain", data = meta_RR)
metareg_all
```

The results of the fit show the mean of the absolute values of the effect sizes as `r round(metareg_all$beta[1],3)` (95% CI [`r round(metareg_all$ci.lb[1],3)`, `r round(metareg_all$ci.ub[1],3)`]), the year change The slope is `r round(metareg_all$beta[2],3)` (95% CI [`r round(metareg_all$ci.lb[2],3)`, `r round(metareg_all$ci.ub[2],3)`]) and is highly significant. The fit results for the impact factor and precision are not significant, but the impact factor still has mean `r round(metareg_all$beta[3],3)` (95% CI [`r round(metareg_all$ci.lb[3],3)`,`r round(metareg_all$ci.ub[3],3)` ]), which means that for every 1 increase in the impact factor, the absolute value of the average effect size of the host study increases by 0.02.

### 7. Conclusion

1.  From Fig \@ref(fig:fig2), Fig \@ref(fig:fig3) and Fig \@ref(fig:fig17), it can be seen that the different data measurement methods largely affect the response values and sampling variance of the experiments. If two different data measurement methods are used in the data set for calculating the response ratio, the final results will obviously deviate from the true values.\
2.  From Fig \@ref(fig:fig9), Fig \@ref(fig:fig10) and Table \@ref(tab:table3), we found that both the climate of the species and the life stage of the species affect the mean size and direction of the effect values. Combined with Fig \@ref(fig:fig4) and Fig \@ref(fig:fig5), we found that almost all studies involved only one climate and one life stage, and this singularity of experimental materials and experimental design led to more biased results.\
3.  from Fig \@ref(fig:fig11) and Fig \@ref(fig:fig12) we find that the number of studies tends to be higher in study areas with larger mean absolute effect values, which confirms the existence of study area preference in ocean acidification studies.\
4.  As seen in Fig \@ref(fig:fig15) and Fig \@ref(fig:fig16), experiments with lower precision (i.e., large sampling variance) tend to have larger effect values. This reflects the file-drawer situation of the study, i.e., experiments with smaller effect values and lower precision tend not to be published. In the case of low precision, only "compelling" experiments are published.\
5.  As seen in Fig \@ref(fig:fig18) and Fig \@ref(fig:fig19), the effect sizes show a decreasing trend from year to year, and this trend is more obvious in the absolute values of effect sizes. Combined with the meta-analysis of Clement et. al. (2022), it can be concluded that the effect sizes found are decreasing due to the study of cold-water and adult fish species, while the previous experiments used more warm-water species and juveniles, which are more sensitive to CO2 changes.\
6.  Fig \@ref(fig:fig20) reveals the proportion of articles with different effect sizes in different impact factor intervals. It is clear that articles with no effect values are more often published in journals with smaller impact factors, while articles with significant effect values are more often published in journals with larger impact factors. We believe that this is a bias in the acceptance of articles by journals and that researchers tend to believe that articles with more significant effect sizes should be published in journals with higher impact factors.\
7.  It can be observed from Fig \@ref(fig:fig21) and Fig \@ref(fig:fig22) that the precision of studies published in them does not increase as the impact factor increases, and the number of study instruments and species used does not change significantly. We believe that this is not compatible with the gradually increasing impact of the journal.

In conjunction with Clement et. al. (2022) and others, we suggest that researchers and journal publishers should try to avoid publication bias and focus more on experimental accuracy, scope, and richness of experimental tools, and should maintain transparency, data availability, and reproducibility of published studies. Researchers should always think critically in the face of early exaggerated effects, and should not just blindly replicate the conditions of previous studies, but should pay attention to whether the effect is generalizable.
